{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8daa8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(\"data_toprocess\")\n",
    "\n",
    "\n",
    "def create_id(df, col_names, id_col_names):\n",
    "    df1 = df.copy()\n",
    "    for col_name, id_col_name in zip(col_names, id_col_names):\n",
    "        df2 = pd.DataFrame(df[col_name].unique(), columns=[col_name])\n",
    "        df2[id_col_name] = df2.index + 1\n",
    "        df1 = df1.merge(df2)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f64a423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an_train.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n",
      "iel_train.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n",
      "bah.tsv\n",
      "      language  iso_code gloss  global_id  local_id transcription  \\\n",
      "1874  MnongRlm       NaN  name       1405       NaN           NaN   \n",
      "\n",
      "      cognate_class tokens  notes  \n",
      "1874           2364  n a n    NaN  \n",
      "ura.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n",
      "an_test.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n",
      "ie_test.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n",
      "pn.tsv\n",
      "       language  iso_code gloss  global_id  local_id transcription  \\\n",
      "7102  Wiradhuri  wira1262  NECK        102       NaN           NaN   \n",
      "\n",
      "      cognate_class tokens  notes  \n",
      "7102           7466  n a n    NaN  \n",
      "huon.tsv\n",
      "    language  iso_code         gloss  global_id  local_id transcription  \\\n",
      "214      Ono       NaN  (he) sees me       1409       NaN           NaN   \n",
      "269    Dedua       NaN   (he) stands       1442       NaN           NaN   \n",
      "270   Borong       NaN   (he) stands       1442       NaN           NaN   \n",
      "276     Kube       NaN   (he) stands       1442       NaN           NaN   \n",
      "278     Tobo       NaN   (he) stands       1442       NaN           NaN   \n",
      "\n",
      "     cognate_class tokens  notes  \n",
      "214           7904  n a n    NaN  \n",
      "269           7670  n a n    NaN  \n",
      "270           7670  n a n    NaN  \n",
      "276           7670  n a n    NaN  \n",
      "278           7670  n a n    NaN  \n",
      "aa.tsv\n",
      "         language iso_code          gloss  global_id  local_id transcription  \\\n",
      "4745  Mnong-Rölöm      mng           name         83       NaN           NaN   \n",
      "5887        Karii      pkt  rice (cooked)        102       NaN           NaN   \n",
      "\n",
      "      cognate_class tokens notes  \n",
      "4745           1440  n a n   NaN  \n",
      "5887           1793  n a n   NaN  \n",
      "st.tsv\n",
      "     language  iso_code gloss  global_id  local_id transcription  \\\n",
      "5976  Motuo-M  dakp1242  thou         94       NaN           NaN   \n",
      "\n",
      "      cognate_class tokens  notes  \n",
      "5976           1362  n a n    NaN  \n",
      "rom.tsv\n",
      "Empty DataFrame\n",
      "Columns: [language, iso_code, gloss, global_id, local_id, transcription, cognate_class, tokens, notes]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1199453/1404462886.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[locs]['transcription'] ='nan'\n",
      "/tmp/ipykernel_1199453/1404462886.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[locs]['transcription'] ='nan'\n",
      "/tmp/ipykernel_1199453/1404462886.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[locs]['transcription'] ='nan'\n",
      "/tmp/ipykernel_1199453/1404462886.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[locs]['transcription'] ='nan'\n",
      "/tmp/ipykernel_1199453/1404462886.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[locs]['transcription'] ='nan'\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    f_pth = os.path.join('data_toprocess',file)\n",
    "    df = pd.read_csv(f_pth, sep='\\t')\n",
    "    df.dropna(subset=['language'], inplace=True)\n",
    "    if 'Unnamed' in df.columns[0]:\n",
    "        df.drop(columns=[df.columns[0]], inplace= True)\n",
    "    print(file)\n",
    "    headers = ['language', 'iso_code','gloss','global_id',\\\n",
    "                'local_id','transcription','cognate_class', 'tokens', 'notes']\n",
    "    for h,c in zip(headers, df.columns):\n",
    "        try:\n",
    "            assert(h == c)\n",
    "        except:\n",
    "            print(h,c)\n",
    "    type_dict = {'global_id': 'int32', 'cognate_class': 'int32'}\n",
    "    \n",
    "    if df['global_id'].isna().any():\n",
    "        df1 = pd.DataFrame(df['gloss'].unique(), columns=['gloss'])\n",
    "        df1['global_id1'] = df1.index + 1\n",
    "        df = df.merge(df1)\n",
    "        df['global_id'] = df['global_id1']\n",
    "        df.drop(columns=['global_id1'], inplace= True)\n",
    "    if not df['global_id'].isna().any():\n",
    "        df = df.astype(type_dict)\n",
    "        df.to_csv(f_pth,sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1343fc5",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "088fbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "files = ['chinese_1964', 'tujia', 'huon', 'aa', 'an_test', 'bah', 'ie_test', 'pn', 'rom', 'st', 'ura']\n",
    "\n",
    "file_map = {'chinese_1964': 'D_test_Chinese-180-18_test.csv',\n",
    "            'tujia': 'D_test_Tujia-109-5_test.csv',\n",
    "            'huon': 'D_test_Huon-140-14_test.csv',\n",
    "            'aa': 'D_test_AustroAsiatic-58-200_test.csv',\n",
    "            'an_test': 'D_test_Austronesian-45-210_test.csv',\n",
    "            'bah': 'D_test_Bahnaric-200-24_test.csv',\n",
    "            'ie_test': 'D_test_IndoEuropean-42-208_test.csv',\n",
    "            'pn': 'D_test_PamaNyungan-67-183_test.csv',\n",
    "            'rom': 'D_test_Romance-110-43_test.csv',\n",
    "            'st': 'D_test_SinoTibetan-64-110_test.csv',\n",
    "            'ura': 'D_test_Uralic-173-8_test.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "498441a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese_1964 18\n",
      "chinese_1964 18\n",
      "chinese_1964 18\n",
      "chinese_1964 18\n",
      "chinese_1964 18\n"
     ]
    }
   ],
   "source": [
    "for split in range(5):\n",
    "    src_pth = os.path.join('splits_source','prop_50', f\"split_{split}\")\n",
    "    for file in files:\n",
    "        train_src = os.path.join(src_pth, 'train', file_map[file].replace('_test.csv', '_train.csv'))\n",
    "        test_src = os.path.join(src_pth, 'test', f\"{file_map[file]}\")\n",
    "        file_pth = os.path.join('data_toprocess',file+'.tsv')\n",
    "        train_src_df = pd.read_csv(train_src, sep='\\t')\n",
    "        test_src_df = pd.read_csv(test_src, sep='\\t')\n",
    "        file_df = pd.read_csv(file_pth, sep= '\\t')\n",
    "        file_df['transcription'] = file_df['transcription'].fillna('nan ')\n",
    "        train_gloss = pd.DataFrame(train_src_df['CONCEPT'].unique(), columns=['gloss'])\n",
    "        test_gloss = pd.DataFrame(test_src_df['CONCEPT'].unique(), columns=['gloss'])\n",
    "        train_tar_df = file_df.merge(train_gloss)\n",
    "        test_tar_df = file_df.merge(test_gloss)\n",
    "        try:\n",
    "            assert(len(file_df) == len(train_tar_df) + len(test_tar_df))\n",
    "        except:\n",
    "            print(file,len(file_df) - (len(train_tar_df) + len(test_tar_df)) )\n",
    "        \n",
    "        train_tar_df.to_csv(os.path.join('datasets',f\"{file}_prop_50_{split}_train.tsv\"), sep= '\\t', index= False)\n",
    "        test_tar_df.to_csv(os.path.join('datasets',f\"{file}_prop_50_{split}_test.tsv\"), sep= '\\t', index= False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cbec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
